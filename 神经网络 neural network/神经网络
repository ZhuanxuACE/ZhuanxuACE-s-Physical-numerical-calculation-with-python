import numpy as np
import matplotlib.pyplot as plt
import creationdataandplot as cp#在此文件下
import copy

#神经网络的层数和每层神经元
NETWORK_SHAPE = [2,3,4,2]
BATCH_SIZE = 100
LEARNING_RATE = 0.01

#1.标准化函数
def normalize(array):
    max_number = np.max(np.abs(array),axis = 1,keepdims = True)
    scale_rate = np.where(max_number == 0, 0, 1/max_number)
    norm = array * scale_rate
    return norm
#向量标准化函数
def vector_normalize(array):
    max_number = np.max(np.abs(array))
    scale_rate = np.where(max_number == 0, 0, 1/max_number)
    norm = array * scale_rate
    return norm

#2.激活函数(第1:n-1层的激活函数)
def activation_ReLU(inputs):
    return np.maximum(0,inputs)

#3.分类函数
def classify(inputs):
    classification = np.rint(inputs[:,1])
    return classification

#4.softmax激活函数(第n层的激活函数)
def activation_softmax(inputs):
    max_values = np.max(inputs,axis = 1,keepdims=True)
    slided_inputs = inputs - max_values
    exp_value =np.exp(slided_inputs)
    norm_base = np.sum(exp_value,axis = 1,keepdims = True)
    norm_values = exp_value/norm_base
    return norm_values

#5.损失函数1
def precive_loss_function(predicted, real):
    real_matrix = np.zeros((len(predicted), 2))
    real_matrix[:,1] = real
    real_matrix[:,0] = 1 - real
    product = np.sum(predicted*real_matrix,axis=1)
    return 1 - product

#5.1.损失函数2
def loss_function(predicted, real):
    condition = (predicted > 0.5)
    binary_predicted = np.where(condition,1,0)
    real_matrix = np.zeros((len(real), 2))
    real_matrix[:,1] = real
    real_matrix[:,0] = 1 - real
    product = np.sum(predicted*real_matrix,axis=1)
    return 1 - product



#6.需求函数(反向传播算法的起点) 
def get_final_layer_perAct_demands(predicted_values,target_vector):#target_vector也是真实值real
    target = np.zeros((len(target_vector),2))
    target[:,1] = target_vector
    target[:,0] = 1 - target_vector
    for i in range(len(target_vector)):
        if np.dot(target[i],predicted_values[i]) > 0.5:
            #不需要变则返回0  
            target[i] = np.array([0,0])
        else:
            target[i] = (target[i] - 0.5)*2
    return target
    
#定义一个层类
class Layer:
    def __init__(self,n_inputs,n_neurons):
        self.weights = np.random.randn(n_inputs,n_neurons)
        self. biases = np.random.randn(n_neurons)  
        
    def layer_forword(self,inputs):
        sum1 = np.dot(inputs,self.weights) + self.biases
        return sum1
    
    def layer_backward(self,preWeights_values, afterWeights_demand):
        preWeights_demands = np.dot(afterWeights_demand,self.weights.T)
        condition = (preWeights_values > 0)
        
        #激活函数导数函数
        value_derivatives = np.where(condition,1,0)
        perActs_demands = value_derivatives*preWeights_demands
        norm_preAct_demands = normalize(perActs_demands)
        weight_adjust_matrix = self.get_weight_adjust_matrix(preWeights_values,afterWeights_demand)
        norm_weight_adjust_matrix = normalize(weight_adjust_matrix)
        return (norm_preAct_demands,norm_weight_adjust_matrix)

    #本层需求值和前一层的输入值相乘
    def get_weight_adjust_matrix(self,preWeight_values,AftWeights_demands):
        plain_weight = np.full(self.weights.shape,1)
        weights_adjust_matrix = np.full(self.weights.shape,0.)
        plain_weight_T = plain_weight.T
        
        for i in range(BATCH_SIZE):
            weights_adjust_matrix += (plain_weight_T*preWeight_values[i,:]).T*AftWeights_demands[i,:]
        weights_adjust_matrix = weights_adjust_matrix/BATCH_SIZE
        return weights_adjust_matrix

#定义一个神经网络类
class Network:
    def __init__(self,network_shape):
        self.shape = network_shape
        self.layers = []
        for i in range(len(network_shape)-1):
            layer = Layer(network_shape[i],network_shape[i+1])
            self.layers.append(layer)
    
    #前馈运算函数
    def network_forward(self,inputs):
        outputs = [inputs]
        for i in range(len(self.layers)):
            layer_sums = self.layers[i].layer_forword(outputs[i])
            if i < len(self.layers) - 1:
                layer_output = activation_ReLU(layer_sums)
                layer_output = normalize(layer_output)
            else:
                layer_output = activation_softmax(layer_sums) 
            outputs.append(layer_output)
        return outputs
    
    #反向传播函数
    def network_backward(self,layer_outputs,target_vector):
        backup_network = copy.deepcopy(self)#备用网络
        #求需求值函数
        preAct_demands = get_final_layer_perAct_demands(layer_outputs[-1],target_vector)
        for i in range(len(self.layers)):
            layer = backup_network.layers[len(self.layers) - (1 + i)]
            if i != 0:
               layer.biases += LEARNING_RATE*np.mean(preAct_demands,axis = 0)
               layer.biases = vector_normalize(layer.biases)
            outputs = layer_outputs[len(layer_outputs) - (2 + i)]
            result_list = layer.layer_backward(outputs,preAct_demands)
            preAct_demands = result_list[0]
            weight_adjust_matrix = result_list=[1]
            layer.weights += LEARNING_RATE * np.float64(weight_adjust_matrix)
            layer.weights = normalize(layer.weights)
            
        return backup_network
    
    #单批次训练
    def one_batch_train(self, batch):
        inputs = batch[:,(0,1)]
        targets = copy.deepcopy(batch[:,2]).astype(int) #标准答案
        outputs = self.network_forward(inputs)
        precise_loss = precive_loss_function(outputs[-1], targets)
        loss = loss_function(outputs[-1],targets)

        if np.mean(precise_loss) <= 0.1:
            print("no need for training")
        else:
            backup_network = self.network_backward(outputs,targets)
            backup_outputs = backup_network.network_forward(inputs)            
            backup_prescise_loss = precive_loss_function(backup_outputs[-1],targets)
            backup_loss = loss_function(backup_outputs[-1],targets)

            #把老网络更新为新网络
            if np.mean(precise_loss) >= np.mean(backup_prescise_loss) or np.mean(loss) >= np.mean(backup_loss): 
                for i in range(len(self.layers)):
                    self.layers[i].weights = backup_network.layers[i].weights.copy()
                    self.layers[i].biases = backup_network.layers[i].biases.copy()
                    print("Improved")
                    
            else:
                print("No imporvement")
        print("---------------------------------------")

#程序从这里开始运行
def main():
    data = cp.creat_data(100)
    
    #选择起始网络
    use_this_network = 'n' #No
    while use_this_network != "Y" and use_this_network != "y":
        network = Network(NETWORK_SHAPE)
        inputs = data[:,(0,1)]
        outputs = network.network_forward(inputs)
        classification = classify(outputs[-1])
        data[:,2] = classification
        cp.plot_data(data,"Before training")
        use_this_network = input("Use this network? Y to yes, N to no")
     
    do_train = input("Train? Y to yes, N to no")
    while do_train == "Y" or do_train == "y" or do_train.isnumeric() == True:
        if do_train.isnumeric() == True:
            n_entries = int(do_train)
        else:
            n_entries = input("Enter the number of data entries used to train. \n")
        
        network.train(n_entries)
        do_train = input("Train? Y to yes, N to no")

    # #导出点的位置和标签，点在圈内为0，在圈外为1
    # data = cp.creat_data(BATCH_SIZE)
    # data1 = data[:,0:2]
    # inputs = data[:,(0,1)]
    # targets = np.copy(data[:,2])#标准答案,真实值
    # cp.plot_data(data,'stander')
    # print(data)
    # print("************************************")
    
    # #创建一个神经网络
    # network = Network(NETWORK_SHAPE)
    # network.one_batch_train(data)

    # out = network.network_forward(data1)
    # classification = classify(out[-1])
    # print(out[-1])
    # print(classify(out[-1]))
    # data[:,2] = classification
    # print(data)
    # cp.plot_data(data,"before_training")
    
    # backup_network = network.network_backward(out,targets)
    # new_outputs = backup_network.network_forward(inputs)
    # new_classification = classify(new_outputs[-1])
    # print(new_classification)
    # data[:,2] = new_classification
    
    # cp.plot_data(data,"after_training")

    # #损失函数输入和输出
    # loss = precive_loss_function(out[-1],data[:,2])
    # data[:,2] = classify(out[-1])
    # print(loss)
    
    # #需求函数输入和输出
    # demand = get_final_layer_perAct_demands(out[-1],targets)
    # print(demand)    
    
    # #调整矩阵,正的需要增大，负的需要减小
    # adjust_matrix = network.layers[-1].get_weight_adjust_matrix(out[-2],demand)
    # print(adjust_matrix)
    # cp.plot_data(data,'speculation')
    
    # #层反向传播
    # layer_backward = network.layers[-1].layer_backward(out[-2],demand)
    # print(layer_backward)
    
main()
