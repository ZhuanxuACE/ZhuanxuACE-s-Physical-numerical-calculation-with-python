import numpy as np
import matplotlib.pyplot as plt
import creationdataandplot as cp
import copy

NETWORK_SHAPE = [2,150,50,2]#层结构
DATA_NUM = 100 #单次训练数据个数
LEARNING_RATE = 0.001 #学习率，0.001效果不错

#正向传播*************************************************************************
#标准化函数，每列的最大绝对值将变为1
def normalize(array):
    max_number = np.max(np.abs(array),axis = 1,keepdims = True)
    scale_rate = np.where(max_number == 0, 0, 1/max_number)
    norm = array * scale_rate
    return norm

#向量标准化函数
def vector_normalize(array):
    max_number = np.max(np.abs(array))
    scale_rate = np.where(max_number == 0, 0, 1/max_number)
    norm = array * scale_rate
    return norm

#激活函数
def activation_ReLU(inputs):
    return np.maximum(0,inputs)

#softmax激活函数(最后一层的激活函数)
def activation_softmax(inputs):
    max_values = np.max(inputs,axis = 1,keepdims=True)
    slided_inputs = inputs - max_values
    exp_value =np.exp(slided_inputs)
    norm_base = np.sum(exp_value,axis = 1,keepdims = True)
    norm_values = exp_value/norm_base
    return norm_values
 
class Layer:
    def __init__(self,n_inputs,n_neurons):
        #定义当前层的权重矩阵
        self.weights = np.random.randn(n_inputs,n_neurons)
        #定义当前层的偏置值
        self. biases = np.random.randn(n_neurons)
        
    def layer_forword(self,inputs):
        #计算当前层的输出结果
        sum1 = np.dot(inputs,self.weights) + self.biases
        return sum1
    
    #当前层的反向传播
    def layer_backward(self, preWeights_values,afterWeights_demands):
        preWeights_demands = np.dot(afterWeights_demands,self.weights.T)
        
        condition = (preWeights_values > 0)
        value_derivatives = np.where(condition, 1, 0)
        preActs_demands = value_derivatives * preWeights_demands
        norm_preAct_demands = normalize(preActs_demands)
        
        weight_adjust_matrix = self.get_weight_adjust_matrix(preWeights_values,afterWeights_demands)
        norm_weight_adjust_matrix = normalize(weight_adjust_matrix)
        
        return (norm_preAct_demands, norm_weight_adjust_matrix)
    
    #计算当前层的调整矩阵
    def get_weight_adjust_matrix(self,preWeights_values, aftWeights_demands):
        plain_weights = np.full(self.weights.shape,1)
        weights_adjust_matrix = np.full(self.weights.shape,0)
        plain_weights_T = plain_weights.T
        
        for i in range(DATA_NUM):
            weights_adjust_matrix = weights_adjust_matrix + (plain_weights_T * preWeights_values[i,:]).T * aftWeights_demands[i,:]
        weights_adjust_matrix = weights_adjust_matrix/DATA_NUM
        return weights_adjust_matrix
    
class Network:
    def __init__(self,network_shape):
        #调用Layer类，生成神经网络的对象
        self.shape = network_shape
        self.layers = []
        for i in range(len(network_shape)-1):
            layer = Layer(network_shape[i],network_shape[i+1])
            self.layers.append(layer)
    
    def network_forward(self,inputs):
        outputs = [inputs]
        for i in range(len(self.layers)):
            #计算第i层
            layer_sums = self.layers[i].layer_forword(outputs[i])
            
            #前n-1层用Relu激活函数，最后一层用softmax激活函数
            if i < len(self.layers) - 1:
                layer_output = activation_ReLU(layer_sums)
                layer_output = normalize(layer_output)
            else:
                layer_output = activation_softmax(layer_sums)
            #将当前层的输出储存在outputs列表里面
            outputs.append(layer_output)
        return outputs
    
    def network_backward(self,layer_outputs, target_vector):
        backup_network = copy.deepcopy(self)
        preAct_demands = get_final_layer_preAct_damands(layer_outputs[-1],target_vector)
        for i in range(len(self.layers)):
            layer = backup_network.layers[len(self.layers) - (1+i)]
            if i != 0:
                layer.biases += LEARNING_RATE * np.mean(preAct_demands, axis = 0)
                layer.biases = vector_normalize(layer.biases)
                
            outputs = layer_outputs[len(layer_outputs) - (2+i)]
            result_list = layer.layer_backward(outputs, preAct_demands)
            preAct_demands = result_list[0]
            weight_adjust_matrix = result_list[1]
            layer.weights += LEARNING_RATE * weight_adjust_matrix
            layer.weights = normalize(layer.weights)
            
        return backup_network
    
    #单批次训练
    def one_batch_train(self, batch):
        #bathc就是输入的数据
        inputs = batch[:,(0,1)]
        standard_answer = copy.deepcopy(batch[:,2].astype(int))
        outputs = self.network_forward(inputs)
        precise_loss = precise_loss_function(outputs[-1],standard_answer)
        loss = loss_function(outputs[-1],standard_answer)

        if np.mean(precise_loss) <= 0.1:
            print("no need for training")
        else:
            backup_network = self.network_backward(outputs,standard_answer)
            backup_outputs = backup_network.network_forward(inputs)
            backup_precise_loss = precise_loss_function(backup_outputs[-1], standard_answer)
            backup_loss = loss_function(backup_outputs[-1], standard_answer)

            if np.mean(precise_loss) >= np.mean(backup_precise_loss) or np.mean(loss) >= np.mean(backup_loss):
                for i in range(len(self.layers)):
                    self.layers[i].weights = backup_network.layers[i].weights.copy()
                    self.layers[i].biases = backup_network.layers[i].biases.copy()
                print("changed")

            else:
                print("no changed")
                
        print("----------------------------------------------------")

    #多批次训练
    def train(self, n_entries):
        n_batches = np.int16(np.ceil(n_entries/DATA_NUM))
        for i in range(n_batches):
            batch = cp.creat_data(DATA_NUM)
            self.one_batch_train(batch)
        
        data = cp.creat_data(100)
        cp.plot_data(data,"Right classification")
        inputs = data[:,0:2]
        outputs = self.network_forward(inputs)
        classification = classify(outputs[-1])
        data[:,2] = classification
        cp.plot_data(data,"after classification")


#反向传播*************************************************************************
#损失函数
def precise_loss_function(predicted,real):
    #predicted为预测值，real为真实值原来为1列，现在变两列，第二列为real
    real_matrix = np.zeros((len(real),2))
    real_matrix[:,1] = real
    real_matrix[:,0] = 1 - real
    #product交叉熵，越大，预测值越接近真实值，越大越准
    product = np.sum(predicted*real_matrix, axis = 1)
    
    #返回为损失函数，越小越准
    return 1 - product

#第二种损失函数
def loss_function(predicted,real):
    condition = (predicted > 0.5)
    binary_predicted = np.where(condition,1,0)
    real_matrix = np.zeros((len(real),2))
    real_matrix[:,1] = real
    real_matrix[:,0] = 1 - real
    product = np.sum(predicted*real_matrix, axis = 1)
    return 1 - product

#需求函数,返回值1则需增加，返回-1则减少，0不变
def get_final_layer_preAct_damands(predicted_value, target_vector):
    target = np.zeros((len(target_vector),2))
    target[:,1] = target_vector
    target[:,0] = 1 - target_vector
    
    for i in range(len(target_vector)):
        if np.dot(target[i],predicted_value[i]) > 0.5:
            target[i] = np.array([0,0])#不调整
        else:
            #返回[1,-1]或[-1,1]
            target[i] = (target[i] - 0.5)*2
    return target

def classify(probabilities):
    #四舍五入,大于0.5的认为和真实值一样
    classification = np.int16(np.rint(probabilities[:,1]))
    return classification

def main():
    #data前两列为xy坐标，最后一列判断是否在圈内
    data = cp.creat_data(DATA_NUM)

    network = Network(NETWORK_SHAPE)
    inputs = data[:,0:2]
    outputs = network.network_forward(inputs)
    classification = classify(outputs[-1])
    data[:,2] = classification
    cp.plot_data(data,"before training")
    while True:
        n_entries = int(input("Enter the number of data entries used to train.\n"))
        network.train(n_entries)
        print(network.layers[-1])

main()
